{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dulhara79/ddpm/blob/main/Diffusion_Models_%7C_PyTorch_Implementation_YT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qd81zYvBKCu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "                    level=logging.INFO, datefmt=\"%I:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xktMP-3Q6LAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e04cfda-65fe-4040-f9b1-d33c486a9b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DDPM"
      ],
      "metadata": {
        "id": "X6Yzoz8sk7t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion:\n",
        "  def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
        "    self.noise_steps=noise_steps\n",
        "    self.beta_start=beta_start\n",
        "    self.beta_end=beta_end\n",
        "    self.img_size=img_size\n",
        "    self.device=device\n",
        "\n",
        "    self.beta=self.prepare_noise_schedule().to(device)\n",
        "    self.alpha=1.-self.beta\n",
        "    self.alpha_hat=torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "  def prepare_noise_schedule(self):\n",
        "    return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
        "\n",
        "  def noise_images(self, x, t):\n",
        "    sqrt_alpha_hat=torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "    sqrt_one_minus_alpha_hat=torch.sqrt(1-self.alpha_hat[t])[:, None, None, None]\n",
        "    epsilon=torch.randn_like(x)\n",
        "    return sqrt_alpha_hat*x+sqrt_one_minus_alpha_hat*epsilon,epsilon\n",
        "\n",
        "  def sample_timesteps(self, n):\n",
        "    return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
        "\n",
        "  def sample(self, model, n):\n",
        "    logging.info(f\"Sampling {n} new images....\")\n",
        "    print(f\"Sampling {n} new images....\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      x=torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "      for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "        t=(torch.ones(n)*i).long().to(self.device)\n",
        "        predicted_noise=model(x, t)\n",
        "        alpha=self.alpha[t][:, None, None, None]\n",
        "        alpha_hat=self.alpha_hat[t][:, None, None, None]\n",
        "        beta=self.beta[t][:, None, None, None]\n",
        "        if i>1:\n",
        "          noise=torch.randn_like(x)\n",
        "        else:\n",
        "          noise=torch.zeros_like(x)\n",
        "        x=1/torch.sqrt(alpha)*(x-((1-alpha)/(torch.sqrt(1-alpha_hat)))*predicted_noise)+torch.sqrt(beta)*noise\n",
        "    model.train()\n",
        "    x=(x.clamp(-1, 1)+1)/2\n",
        "    x=(x*255).type(torch.uint8)\n",
        "    return x"
      ],
      "metadata": {
        "id": "JHbPfMrYGbY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(args):\n",
        "#   setup_logging(args.run_name)\n",
        "#   device=args.device\n",
        "#   dataloader=get_data(args)\n",
        "#   model=UNet().to(device)\n",
        "#   optimizer=optim.AdamW(model.parameters(), lr=args.lr)\n",
        "#   mse=nn.MSELoss()\n",
        "#   diffusion=Diffusion(img_size=args.image_size, device=device)\n",
        "#   logger=SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
        "#   l = len(dataloader)\n",
        "\n",
        "#   for epoch in range(args.epochs):\n",
        "#     logging.info(f\"Starting epoch {epoch}:\")\n",
        "#     pbar=tqdm(dataloader)\n",
        "#     for i, (images, _) in enumerate(pbar):\n",
        "#       images=images.to(device)\n",
        "#       t=diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "#       x_t, noise=diffusion.noise_images(images, t)\n",
        "#       predicted_noice=model(x_t, t)\n",
        "#       loss=mse(noise, predicted_noice)\n",
        "\n",
        "#       optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "\n",
        "#       pbar.set_postfix(MSE=loss.item())\n",
        "#       logger.add_scalar(\"MSE\", loss.item(), global_step=epoch*l+i)\n",
        "\n",
        "#     sampled_images=diffusion.sample(model, n=images.shape[0])\n",
        "#     save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
        "#     torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))"
      ],
      "metadata": {
        "id": "w_O_k45TV93x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    setup_logging(args.run_name)\n",
        "    device = args.device\n",
        "    dataloader = get_data(args)\n",
        "    model = UNet().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "    model_path = os.path.join(\"models\", args.run_name, \"ckpt.pt\")\n",
        "    start_epoch = 0\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        logging.info(\"Loading checkpoint...\")\n",
        "        print(\"Loading checkpoint...\")\n",
        "        checkpoint = torch.load(model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        # Typo fixed: \"Resuming\"\n",
        "        logging.info(f\"Resuming training from epoch {start_epoch}\")\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    mse = nn.MSELoss()\n",
        "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
        "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
        "    l = len(dataloader)\n",
        "\n",
        "    # FIX #2: The loop now starts from the correct epoch\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        logging.info(f\"Starting epoch {epoch}:\")\n",
        "        print(f\"Starting epoch {epoch}:\")\n",
        "        pbar = tqdm(dataloader)\n",
        "        for i, (images, _) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "            x_t, noise = diffusion.noise_images(images, t)\n",
        "            predicted_noise = model(x_t, t)\n",
        "            loss = mse(noise, predicted_noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix(MSE=loss.item())\n",
        "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "        # After the epoch is done, generate sample images\n",
        "        sampled_images = diffusion.sample(model, n=images.shape[0])\n",
        "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
        "\n",
        "        # FIX #1 & #3: Save the checkpoint ONCE per epoch and remove the conflicting save\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "        }, model_path)\n",
        "        logging.info(f\"Epoch {epoch} finished and checkpoint saved.\")\n",
        "        print(f\"Epoch {epoch} finished and checkpoint saved.\")"
      ],
      "metadata": {
        "id": "7-wxCTuNepcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import arg\n",
        "def launch():\n",
        "  import argparse\n",
        "  parser=argparse.ArgumentParser()\n",
        "  args=parser.parse_args([])\n",
        "  args.run_name=\"DDPM_Unconditional\"\n",
        "  args.epochs=500\n",
        "  args.batch_size=8\n",
        "  args.image_size=64\n",
        "  # primary\n",
        "  # args.dataset_path=r\"/content/drive/MyDrive/Colab Notebooks/MLOM/MLOM_Assignment-1/landscape_pictures\"\n",
        "  # nwatch\n",
        "  # args.dataset_path=r\"/content/drive/MyDrive/Colab Notebooks/landscape_pictures\"\n",
        "  # args.dataset_path=r\"/content/drive/MyDrive/MLOM/landscape/MLOM_Assignment-1\"\n",
        "  # kaushalyadulhara\n",
        "  args.dataset_path=r\"/content/drive/MyDrive/MLOM-Assignment/landscape_pictures\"\n",
        "  args.device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  args.lr=3e-4\n",
        "\n",
        "  if not os.path.exists(args.dataset_path):\n",
        "        logging.error(f\"Dataset path not found: {args.dataset_path}. Please upload a directory named 'dataset' with subfolders containing images.\")\n",
        "\n",
        "  train(args)"
      ],
      "metadata": {
        "id": "CrOOxJwVbFP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modules"
      ],
      "metadata": {
        "id": "ndgMVZVYlFRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "  def __init__(self, beta):\n",
        "    super().__init__()\n",
        "    self.beta=beta\n",
        "    self.step=0\n",
        "\n",
        "  def update_model_avarage(self, ma_model, current_model):\n",
        "    for current_params, ma_params in zip(current_model.parameters(),\n",
        "                                         ma_model.parameters()):\n",
        "      old_weight, up_weight=ma_params.data, current_params.data\n",
        "      ma_params.data=self.update_avarage(old_weight, up_weight)\n",
        "\n",
        "  def update_avarage(self, old, new):\n",
        "    if old is None:\n",
        "      return new\n",
        "    return old*self.beta+(1-self.beta)*new\n",
        "\n",
        "  def step_ema(self, ema_model, model, step_start_ema=2000):\n",
        "    if self.step<step_start_ema:\n",
        "      self.reset_parameters(ema_model, model)\n",
        "      self.step+=1\n",
        "      return\n",
        "\n",
        "    self.update_model_avarage(ema_model, model)\n",
        "    self.step+=1\n",
        "\n",
        "  def reset_parameters(self, ema_model, model):\n",
        "    ema_model.load_state_dict(model.state_dict())"
      ],
      "metadata": {
        "id": "O174cQSGlC6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, channels, size):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.channels=channels\n",
        "    self.size=size\n",
        "    self.mha=nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "    self.ln=nn.LayerNorm([channels])\n",
        "    self.ff_self=nn.Sequential(\n",
        "        nn.LayerNorm([channels]),\n",
        "        nn.Linear(channels, channels),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(channels, channels),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=x.view(-1, self.channels, self.size*self.size).swapaxes(1, 2)\n",
        "    x_ln=self.ln(x)\n",
        "    attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "    attention_value=attention_value+x\n",
        "    attention_value=self.ff_self(attention_value)+attention_value\n",
        "    return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size,\n",
        "                                               self.size)"
      ],
      "metadata": {
        "id": "5KnuJugLnCU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "    super().__init__()\n",
        "    self.residual=residual\n",
        "    if not mid_channels:\n",
        "      mid_channels=out_channels\n",
        "    self.double_conv=nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.GroupNorm(1, mid_channels),\n",
        "        nn.GELU(),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.GroupNorm(1, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.residual:\n",
        "      return F.gelu(x+self.double_conv(x))\n",
        "    else:\n",
        "      return self.double_conv(x)"
      ],
      "metadata": {
        "id": "PpCZAcQBo21C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv=nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, in_channels, residual=True),\n",
        "        DoubleConv(in_channels, out_channels),\n",
        "    )\n",
        "    self.emb_layer=nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(\n",
        "            emb_dim,\n",
        "            out_channels\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    x=self.maxpool_conv(x)\n",
        "    emb=self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2],\n",
        "                                                   x.shape[-1])\n",
        "    return x+emb"
      ],
      "metadata": {
        "id": "IFhrz4rmqoPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2],\n",
        "                                                         x.shape[-1])\n",
        "        return x + emb"
      ],
      "metadata": {
        "id": "FUOdGbwx_j3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
        "    super().__init__()\n",
        "    self.device=device\n",
        "    self.time_dim=time_dim\n",
        "\n",
        "    self.inc=DoubleConv(c_in, 64)\n",
        "\n",
        "    self.down1=Down(64, 128)\n",
        "    self.sa1=SelfAttention(128, 32)\n",
        "    self.down2=Down(128, 256)\n",
        "    self.sa2=SelfAttention(256, 16)\n",
        "    self.down3=Down(256, 256)\n",
        "    self.sa3=SelfAttention(256, 8)\n",
        "\n",
        "    self.bot1=DoubleConv(256, 512)\n",
        "    self.bot2=DoubleConv(512, 512)\n",
        "    self.bot3=DoubleConv(512, 256)\n",
        "\n",
        "    self.up1=Up(512, 128)\n",
        "    self.sa4=SelfAttention(128, 16)\n",
        "\n",
        "    self.up2=Up(256, 64)\n",
        "    self.sa5=SelfAttention(64, 32)\n",
        "\n",
        "    self.up3=Up(128, 64)\n",
        "    self.sa6=SelfAttention(64, 64)\n",
        "\n",
        "    self.outc=nn.Conv2d(64, c_out, kernel_size=1)\n",
        "\n",
        "  def pos_encoding(self, t, channels):\n",
        "    inv_freq=1.0/(\n",
        "        10000\n",
        "        **(torch.arange(0, channels, 2, device=self.device).float()/channels)\n",
        "        )\n",
        "    pos_enc_a=torch.sin(t.repeat(1, channels//2)*inv_freq)\n",
        "    pos_enc_b=torch.cos(t.repeat(1, channels//2)*inv_freq)\n",
        "    pos_enc=torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    t=t.unsqueeze(-1).type(torch.float)\n",
        "    t=self.pos_encoding(t, self.time_dim)\n",
        "\n",
        "    x1=self.inc(x)\n",
        "    x2=self.down1(x1, t)\n",
        "    x2=self.sa1(x2)\n",
        "    x3=self.down2(x2, t)\n",
        "    x3=self.sa2(x3)\n",
        "    x4=self.down3(x3, t)\n",
        "    x4=self.sa3(x4)\n",
        "\n",
        "    x4=self.bot1(x4)\n",
        "    x4=self.bot2(x4)\n",
        "    x4=self.bot3(x4)\n",
        "\n",
        "    x=self.up1(x4, x3, t)\n",
        "    x=self.sa4(x)\n",
        "    x=self.up2(x, x2, t)\n",
        "    x=self.sa5(x)\n",
        "    x=self.up3(x, x1, t)\n",
        "    x=self.sa6(x)\n",
        "    output=self.outc(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1O-ZsBTksPge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "YPCa1hd04ffK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images):\n",
        "  plt.figure(figsize=(32, 32))\n",
        "  plt.imshow(torch.cat([\n",
        "      torch.cat([i for i in images.cpu()], dim=-1),\n",
        "  ], dim=-2).permute(1, 2, 0).cpu())\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ym-Ojzr-8QFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(images, path, **kwargs):\n",
        "  grid=torchvision.utils.make_grid(images, **kwargs)\n",
        "  ndarr=grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
        "  im=Image.fromarray(ndarr)\n",
        "  im.save(path)"
      ],
      "metadata": {
        "id": "mhbGfADX9I1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(args):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
        "        torchvision.transforms.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    dataset = torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "qocwi3Bo93fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging(run_name):\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"
      ],
      "metadata": {
        "id": "82aaDWha964q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_xjbc38FQMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Make sure to call launch() to start the process\n",
        "    # The UNet is missing the implementation of 'Up' and 'F.gelu' is called without 'F' imported\n",
        "    # Fixes are made in the code above.\n",
        "    try:\n",
        "        launch()\n",
        "    except TypeError as e:\n",
        "        print(f\"An error occurred during launch: {e}\")\n",
        "        print(\"This may be due to the UNet's forward pass not exactly matching the expected channel sizes for the Up blocks after concatenation. Please double-check the channel math.\")"
      ],
      "metadata": {
        "id": "c0E3apV5FQIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "d117c569-80d4-4c5d-ebb6-615d4b841fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Unsupported operand 198\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3178257219.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Fixes are made in the code above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"An error occurred during launch: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1145562973.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset path not found: {args.dataset_path}. Please upload a directory named 'dataset' with subfolders containing images.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2773601569.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading checkpoint...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading checkpoint...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 )\n\u001b[1;32m   1552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         return _legacy_load(\n\u001b[1;32m   1555\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: Unsupported operand 198\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    }
  ]
}